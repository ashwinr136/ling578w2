{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://hammondm.github.io/hltlogo1.png' style=\"float:right\">\n",
    "\n",
    "Linguistics 578<br>\n",
    "Fall 2023<br>\n",
    "Hammond\n",
    "\n",
    "<h3 align=center>\n",
    "Traditional recognition\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "The general topic of this notebook is *traditional speech recognition*.\n",
    "\n",
    "Topics to come:\n",
    "\n",
    "1. General logic of automatic speech recognition\n",
    "1. Improving `tr8.py` (from the book)\n",
    "1. How to compose an HMM-GMM with transducers\n",
    "1. State tying\n",
    "1. Kaldi\n",
    "\n",
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pomegranate as p\n",
    "import numpy as np\n",
    "import librosa,re,os,time,graphviz\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of ASR\n",
    "\n",
    "The general logic of traditional ASR is that we model it as composition of transducers.\n",
    "\n",
    "The first transducer is the *acoustic model*, where we map from acoustic frames to phonetic units like segments. (This notebook focuses on this.)\n",
    "\n",
    "This is composed with at least two more weighted automata. One maps from phonetic elements to word tokens, effectively winnowing the sequences of phonetic elements to actual words in the language.\n",
    "\n",
    "The bottommost weighted transducer is an identity transducer that chooses among different word sequences, effectively winnowing word sequences to the most likely sequence.\n",
    "\n",
    "We call these latter steps the *language model*.\n",
    "\n",
    "![full model](asr.png)\n",
    "\n",
    "We've already played with parts of the language model and so we focus on the acoustic model here.\n",
    "\n",
    "We use the *speech commands* dataset which you can download from [here](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the code from the book\n",
    "\n",
    "Here's the code from `tr8.py` in the book, which implements the acoustic model. This one uses:\n",
    "\n",
    "- Linear HMMs\n",
    "- LPC\n",
    "- Multivariate Gaussians\n",
    "\n",
    "I've tweaked the code in a couple of ways.\n",
    "\n",
    "1. I tweaked how it prints updates.\n",
    "1. I added bits at the beginning and end to time the whole thing.\n",
    "1. I tweaked the parameters for the `fit()` method. Limiting the number of iterations gets *much* better performance, indicating that the models can overfit.\n",
    "\n",
    "Let's run it and then go through the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LPC version tweaked from book code\n",
    "start_time = time.time()\n",
    "\n",
    "order = 8\n",
    "wlength = 300\n",
    "numtrain = 20\n",
    "\n",
    "digits = [\n",
    "    'zero','one','two','three','four',\n",
    "    'five','six','seven','eight','nine'\n",
    "]\n",
    "\n",
    "where = '/mhdata/commands/'\n",
    "\n",
    "#create stored digits\n",
    "allscores = []\n",
    "filelist = []\n",
    "for digit in digits:\n",
    "    digitset = []\n",
    "    files = os.listdir(where+digit)\n",
    "    filelist.append(files)\n",
    "    for f in files:\n",
    "        try:\n",
    "            fs,w = wavfile.read(where + digit + '/' + f)\n",
    "            w = w.astype(float)\n",
    "            cur = 0\n",
    "            res = []\n",
    "            while cur+wlength <= len(w):\n",
    "                lpc = librosa.lpc(\n",
    "                    y=w[cur:cur+wlength],\n",
    "                    order=order\n",
    "                )\n",
    "                res.append(lpc)\n",
    "                cur += wlength\n",
    "            res = np.array(res)\n",
    "            digitset.append(res)\n",
    "        except:\n",
    "            pass\n",
    "        if len(digitset) == numtrain+10: break\n",
    "    allscores.append(digitset)\n",
    "\n",
    "#make linear HMMs\n",
    "print('creating HMMs...')\n",
    "segments = np.array([4,3,2,3,3,4,4,5,3,4,3])\n",
    "lengths = segments*3 + 2\n",
    "hmms = []\n",
    "for i in range(10):\n",
    "    states = lengths[i]\n",
    "    m = p.HiddenMarkovModel('d' + str(i))\n",
    "    #states\n",
    "    statelist = []\n",
    "    for s in range(states):\n",
    "        d = p.MultivariateGaussianDistribution(\n",
    "            np.arange(order),\n",
    "            np.eye(order)\n",
    "        )\n",
    "        s = p.State(d,name='s' + str(s))\n",
    "        statelist.append(s)\n",
    "    m.add_states(statelist)\n",
    "    #start prob\n",
    "    m.add_transition(m.start,statelist[0],1.0)\n",
    "    #final state\n",
    "    #m.add_transition(m.end,statelist[-1],0.5)  \n",
    "    m.add_transition(statelist[-1],m.end,0.5)\n",
    "    \n",
    "    #loop transitions\n",
    "    for state in statelist:\n",
    "        m.add_transition(state,state,0.5)\n",
    "    #sequential transitions\n",
    "    for i in range(len(statelist)-1):\n",
    "        m.add_transition(\n",
    "            statelist[i],\n",
    "            statelist[i+1],\n",
    "            0.5\n",
    "        )\n",
    "    m.bake()\n",
    "    hmms.append(m)\n",
    "\n",
    "#train HMMs\n",
    "print('training...')\n",
    "for i in range(10):\n",
    "    trainset = allscores[i][:numtrain]\n",
    "    hmm = hmms[i]\n",
    "    hmm.fit(\n",
    "        trainset,\n",
    "        n_jobs=-1,\n",
    "        max_iterations=5\n",
    "    )\n",
    "    print(f'\\t{i}')\n",
    "\n",
    "#test HMMs\n",
    "print('testing...')\n",
    "total = 0\n",
    "for i in range(10):\n",
    "    testset = allscores[i][numtrain:numtrain+10]\n",
    "    for testitem in testset:\n",
    "        allres = []\n",
    "        for hmm in hmms:\n",
    "            res = hmm.probability(testitem)\n",
    "            allres.append(res)\n",
    "        allres = np.array(allres)\n",
    "        idx = allres.argmax()\n",
    "        if idx == i: total += 1\n",
    "\n",
    "print(f'Correct: {total}/100')\n",
    "\n",
    "print(f'({time.time() - start_time:.4} seconds)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The book notes that there are several things to be done with this to reach state of the art for HMM-based synthesis:\n",
    "\n",
    "1. use MFCCs\n",
    "1. use CMVN\n",
    "1. use Gaussian mixture models\n",
    "1. use state tying\n",
    "\n",
    "Let's implement some of these here.\n",
    "\n",
    "Let's start with using MFCCs instead of LPC. Rather than implement this from scratch, we'll use the implementation from `librosa`.\n",
    "\n",
    "Let's first take a look at the MFCC steps. First we read in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(where+'zero')\n",
    "fs,w = wavfile.read(where + 'zero/' + files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it to make sure there are no surprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(w)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make an MFCC. There are several bits here to note\n",
    "\n",
    "1. We specify 26 initial MFCC coefficients.\n",
    "1. We specify 25msec windows and 10msec hops.\n",
    "1. We then extract the 2nd through 13th coefficients.\n",
    "1. We have to calculate deltas and delta-deltas separately and then tack them on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w = w.astype(float)\n",
    "mfcc = librosa.feature.mfcc(\n",
    "    y=w,\n",
    "    sr=fs,\n",
    "    n_mfcc=26,\n",
    "    win_length=400,\n",
    "    hop_length=160\n",
    ")\n",
    "mfcc = mfcc[1:13,:]\n",
    "delta = librosa.feature.delta(mfcc)\n",
    "deltadelta = librosa.feature.delta(mfcc,order=2)\n",
    "res = np.vstack([mfcc,delta,deltadelta])\n",
    "plt.plot(res.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following we give a revision of the code above that plugs in MFCCs for LPC. Let's first do it with a minimal HMM to make sure we have the MFCC parts right. The HMMs below have only a single state each.\n",
    "\n",
    "Working this out revealed a couple of important things.\n",
    "\n",
    "1. The GMM part should actually require the syntax below, rather than what we have above. Note that this is still multivariate, but not mixture.\n",
    "1. Probability values are *extremely* small, so we switch to `log_probabbility()` here.\n",
    "\n",
    "With these limits, the system does *not* perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MFCCs, single-node HMMs, no mixtures\n",
    "start_time = time.time()\n",
    "\n",
    "numtrain = 100\n",
    "\n",
    "#create stored digits\n",
    "allscores = []\n",
    "filelist = []\n",
    "for digit in digits:\n",
    "    digitset = []\n",
    "    files = os.listdir(where+digit)\n",
    "    filelist.append(files)\n",
    "    for f in files:\n",
    "        try:\n",
    "            fs,w = wavfile.read(where + digit + '/' + f)\n",
    "            w = w.astype(float)\n",
    "            mfcc = librosa.feature.mfcc(\n",
    "                y=w,\n",
    "                sr=fs,\n",
    "                n_mfcc=26,\n",
    "                win_length=400,\n",
    "                hop_length=160\n",
    "            )\n",
    "            mfcc = mfcc[1:13,:]\n",
    "            delta = librosa.feature.delta(mfcc)\n",
    "            deltadelta = librosa.feature.delta(mfcc,order=2)\n",
    "            res = np.vstack([mfcc,delta,deltadelta])\n",
    "            res = res.T\n",
    "            \n",
    "            digitset.append(res)\n",
    "        except:\n",
    "            pass\n",
    "        if len(digitset) == numtrain+10: break\n",
    "    allscores.append(digitset)\n",
    "\n",
    "#make HMMs\n",
    "print('creating HMMs...')\n",
    "hmms = []\n",
    "for i in range(10):\n",
    "    states = lengths[i]\n",
    "    m = p.HiddenMarkovModel('d' + str(i))\n",
    "    #states\n",
    "    statelist = []\n",
    "    ds = []\n",
    "    for i in range(36):\n",
    "        ds.append(p.NormalDistribution(1,1))\n",
    "    d = p.IndependentComponentsDistribution(ds)\n",
    "        \n",
    "    s = p.State(d,name='s')\n",
    "    statelist.append(s)\n",
    "    m.add_states(statelist)\n",
    "    #start prob\n",
    "    m.add_transition(m.start,statelist[0],1.0)\n",
    "    m.add_transition(statelist[0],statelist[0],0.5)\n",
    "    \n",
    "    #sequential transitions\n",
    "    for i in range(len(statelist)-1):\n",
    "        m.add_transition(\n",
    "            statelist[i],\n",
    "            statelist[i+1],\n",
    "            0.5\n",
    "        )\n",
    "    \n",
    "    #final state\n",
    "    m.add_transition(statelist[-1],m.end,0.5)\n",
    "    \n",
    "    m.bake()\n",
    "    hmms.append(m)\n",
    "\n",
    "#train HMMs\n",
    "print('training...')\n",
    "for i in range(10):\n",
    "    trainset = allscores[i][:numtrain]\n",
    "    hmm = hmms[i]\n",
    "    hmm.fit(\n",
    "        trainset,\n",
    "        n_jobs=-1,\n",
    "        max_iterations=5\n",
    "    )\n",
    "    print(f'\\t{i}')\n",
    "\n",
    "#test HMMs\n",
    "print('testing...')\n",
    "total = 0\n",
    "for i in range(10):\n",
    "    testset = allscores[i][numtrain:numtrain+10]\n",
    "    for testitem in testset:\n",
    "        allres = []\n",
    "        for hmm in hmms:\n",
    "            res = hmm.log_probability(testitem)\n",
    "            allres.append(res)\n",
    "        allres = np.array(allres)\n",
    "        idx = allres.argmax()\n",
    "        if idx == i: total += 1\n",
    "\n",
    "print(f'Correct: {total}/100')\n",
    "\n",
    "print(f'({time.time() - start_time:.4} seconds)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now convert to mixture models. The following code does this. Note that we can specify how many mixtures here.\n",
    "\n",
    "CMVN for files is actually quite simple, so we add that code along with a flag to turn it on and off.\n",
    "\n",
    "These do *not* improve things immediately. In fact, CMVN here decreases performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#MFCCs, single-node HMMs, mixtures, CMVN\n",
    "\n",
    "mixtures = 2\n",
    "\n",
    "cmvn = False\n",
    "\n",
    "numtrain = 100\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#create stored digits\n",
    "allscores = []\n",
    "filelist = []\n",
    "for digit in digits:\n",
    "    digitset = []\n",
    "    files = os.listdir(where+digit)\n",
    "    filelist.append(files)\n",
    "    for f in files:\n",
    "        try:\n",
    "            fs,w = wavfile.read(where + digit + '/' + f)\n",
    "            w = w.astype(float)\n",
    "            mfcc = librosa.feature.mfcc(\n",
    "                y=w,\n",
    "                sr=fs,\n",
    "                n_mfcc=26,\n",
    "                win_length=400,\n",
    "                hop_length=160\n",
    "            )\n",
    "            mfcc = mfcc[1:13,:]\n",
    "            delta = librosa.feature.delta(mfcc)\n",
    "            deltadelta = librosa.feature.delta(mfcc,order=2)\n",
    "            res = np.vstack([mfcc,delta,deltadelta])\n",
    "            res = res.T\n",
    "            \n",
    "            #CMVN bits\n",
    "            if cmvn:\n",
    "                mean = np.mean(res,axis=0)\n",
    "                std = np.std(res,axis=0)\n",
    "                res = (res - mean)/std\n",
    "            \n",
    "            digitset.append(res)\n",
    "        except:\n",
    "            pass\n",
    "        if len(digitset) == numtrain+10: break\n",
    "    allscores.append(digitset)\n",
    "\n",
    "#make HMMs\n",
    "print('creating HMMs...')\n",
    "hmms = []\n",
    "for i in range(10):\n",
    "    #states = lengths[i]\n",
    "    m = p.HiddenMarkovModel('d' + str(i))\n",
    "    #states\n",
    "    statelist = []\n",
    "    \n",
    "    mix = []\n",
    "    for j in range(mixtures):\n",
    "        ds = []\n",
    "        for i in range(36):\n",
    "            ds.append(p.NormalDistribution(1,1))\n",
    "        mix.append(p.IndependentComponentsDistribution(ds))\n",
    "    weights = np.ones(mixtures)\n",
    "    weights *= 1/mixtures\n",
    "    d = p.GeneralMixtureModel(mix,weights=weights)\n",
    "    \n",
    "    s = p.State(d,name='s')\n",
    "    statelist.append(s)\n",
    "    m.add_states(statelist)\n",
    "    #start prob\n",
    "    m.add_transition(m.start,statelist[0],1.0)\n",
    "    m.add_transition(statelist[0],statelist[0],0.5)\n",
    "    \n",
    "    #final state\n",
    "    #m.add_transition(m.end,statelist[-1],0.5)\n",
    "    m.add_transition(statelist[-1],m.end,0.5)\n",
    "\n",
    "    m.bake()\n",
    "    hmms.append(m)\n",
    "\n",
    "#train HMMs\n",
    "print('training...')\n",
    "for i in range(10):\n",
    "    trainset = allscores[i][:numtrain]\n",
    "    hmm = hmms[i]\n",
    "    hmm.fit(\n",
    "        trainset,\n",
    "        n_jobs=-1,\n",
    "        max_iterations=5\n",
    "    )\n",
    "    print(f'\\t{i}')\n",
    "\n",
    "#test HMMs\n",
    "print('testing...')\n",
    "total = 0\n",
    "for i in range(10):\n",
    "    testset = allscores[i][numtrain:numtrain+10]\n",
    "    for testitem in testset:\n",
    "        allres = []\n",
    "        for hmm in hmms:\n",
    "            res = hmm.log_probability(testitem)\n",
    "            allres.append(res)\n",
    "        allres = np.array(allres)\n",
    "        idx = allres.argmax()\n",
    "        if idx == i: total += 1\n",
    "\n",
    "print(f'Correct: {total}/100')\n",
    "\n",
    "print(f'({time.time() - start_time:.4} seconds)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go to linear HMMs. These HMMs are bigger, so we need to train them more. This does slightly better. CMVN doesn't affect this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#MFCCs, linear HMMs, mixtures, CMVN\n",
    "\n",
    "mixtures = 2\n",
    "\n",
    "cmvn = False\n",
    "\n",
    "numtrain = 50\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#create stored digits\n",
    "allscores = []\n",
    "filelist = []\n",
    "for digit in digits:\n",
    "    digitset = []\n",
    "    files = os.listdir(where+digit)\n",
    "    filelist.append(files)\n",
    "    for f in files:\n",
    "        try:\n",
    "            fs,w = wavfile.read(where + digit + '/' + f)\n",
    "            w = w.astype(float)\n",
    "            mfcc = librosa.feature.mfcc(\n",
    "                y=w,\n",
    "                sr=fs,\n",
    "                n_mfcc=26,\n",
    "                win_length=400,\n",
    "                hop_length=160\n",
    "            )\n",
    "            mfcc = mfcc[1:13,:]\n",
    "            delta = librosa.feature.delta(mfcc)\n",
    "            deltadelta = librosa.feature.delta(mfcc,order=2)\n",
    "            res = np.vstack([mfcc,delta,deltadelta])\n",
    "            res = res.T\n",
    "            \n",
    "            #CMVN bits\n",
    "            if cmvn:\n",
    "                mean = np.mean(res,axis=0)\n",
    "                std = np.std(res,axis=0)\n",
    "                res = (res - mean)/std\n",
    "            \n",
    "            digitset.append(res)\n",
    "        except:\n",
    "            pass\n",
    "        if len(digitset) == numtrain+10: break\n",
    "    allscores.append(digitset)\n",
    "\n",
    "#make linear HMMs\n",
    "print('creating HMMs...')\n",
    "segments = np.array([4,3,2,3,3,4,4,5,3,4,3])\n",
    "lengths = segments*3 + 2\n",
    "hmms = []\n",
    "for i in range(10):\n",
    "    states = lengths[i]\n",
    "    m = p.HiddenMarkovModel('d' + str(i))\n",
    "    #states\n",
    "    statelist = []\n",
    "\n",
    "    for s in range(states):\n",
    "        mix = []\n",
    "        for j in range(mixtures):\n",
    "            ds = []\n",
    "            for i in range(36):\n",
    "                ds.append(p.NormalDistribution(1,1))\n",
    "            mix.append(p.IndependentComponentsDistribution(ds))\n",
    "        weights = np.ones(mixtures)\n",
    "        weights *= 1/mixtures\n",
    "        d = p.GeneralMixtureModel(mix,weights=weights)\n",
    "\n",
    "        statelist.append(\n",
    "            p.State(d,name='sk' + str(s))\n",
    "        )\n",
    "\n",
    "    m.add_states(statelist)\n",
    "    #start prob\n",
    "    m.add_transition(m.start,statelist[0],1.0)\n",
    "\n",
    "    #final state\n",
    "    m.add_transition(statelist[-1],m.end,0.5)\n",
    "    \n",
    "    #loop transitions\n",
    "    for state in statelist:\n",
    "        m.add_transition(state,state,0.5)\n",
    "    #sequential transitions\n",
    "    for i in range(len(statelist)-1):\n",
    "        m.add_transition(\n",
    "            statelist[i],\n",
    "            statelist[i+1],\n",
    "            0.5\n",
    "        )\n",
    "        \n",
    "    m.bake()\n",
    "    hmms.append(m)\n",
    "\n",
    "#train HMMs\n",
    "print('training...')\n",
    "for i in range(10):\n",
    "    trainset = allscores[i][:numtrain]\n",
    "    hmm = hmms[i]\n",
    "    hmm.fit(\n",
    "        trainset,\n",
    "        n_jobs=-1,\n",
    "        max_iterations=30\n",
    "    )\n",
    "    print(f'\\t{i}')\n",
    "\n",
    "#test HMMs\n",
    "print('testing...')\n",
    "total = 0\n",
    "for i in range(10):\n",
    "    testset = allscores[i][numtrain:numtrain+10]\n",
    "    for testitem in testset:\n",
    "        allres = []\n",
    "        for hmm in hmms:\n",
    "            res = hmm.log_probability(testitem)\n",
    "            allres.append(res)\n",
    "        allres = np.array(allres)\n",
    "        idx = allres.argmax()\n",
    "        if idx == i: total += 1\n",
    "\n",
    "print(f'Correct: {total}/100')\n",
    "\n",
    "print(f'({time.time() - start_time:.4} seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.849e+03/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing left here is state tying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: composition with an HMM-GMM\n",
    "\n",
    "In an earlier notebook, we showed one way to convert an HMM into a WFSA which can then be composed with other WFSAs/WFSTs.\n",
    "\n",
    "What about an HMM-GMM?\n",
    "\n",
    "The problem is that an HMM-GMM allows us to compute probabilities over an infinite number of possible distinct acoustic frames, but to compose this with other transducers, we need something with a finite number of categories.\n",
    "\n",
    "One solution to this problem is to feed the input into the HMM-GMM *before* composition with the other transducers. We then have a finite number of distinct acoustic frames, namely the frames in the input. We can use those to convert the GMMs to a distribution over a fixed set of frames. This new HMM can then be converted to a WFSA and composed as we described earlier.\n",
    "\n",
    "The cost of this is that we have to do composition for each input. In fact, this is not an issue because we've already seen that, for a system like OpenFST, inputs are handled with composition anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State tying\n",
    "\n",
    "The next missing piece is state tying. The logic of this step is that we treat each segment as a linear HMM of three states. These are combined to make words. For example, a word like *hat* might be represented as something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = graphviz.Digraph(\n",
    "    graph_attr={'rankdir':'LR'}\n",
    ")\n",
    "dot.node('q0',label='#',shape='circle')\n",
    "dot.node('q1',label='h',shape='circle')\n",
    "dot.node('q2',label='h',shape='circle')\n",
    "dot.node('q3',label='h',shape='circle')\n",
    "dot.node('q4',label='æ',shape='circle')\n",
    "dot.node('q5',label='æ',shape='circle')\n",
    "dot.node('q6',label='æ',shape='circle')\n",
    "dot.node('q7',label='t',shape='circle')\n",
    "dot.node('q8',label='t',shape='circle')\n",
    "dot.node('q9',label='t',shape='circle')\n",
    "dot.node('q10',label='#',shape='doublecircle')\n",
    "dot.node('s',style='invisible')\n",
    "dot.edge('s','q0')\n",
    "dot.edge('q0','q0')\n",
    "dot.edge('q0','q1')\n",
    "dot.edge('q1','q1')\n",
    "dot.edge('q1','q2')\n",
    "dot.edge('q2','q2')\n",
    "dot.edge('q2','q3')\n",
    "dot.edge('q3','q3')\n",
    "dot.edge('q3','q4')\n",
    "dot.edge('q4','q4')\n",
    "dot.edge('q4','q5')\n",
    "dot.edge('q5','q5')\n",
    "dot.edge('q5','q6')\n",
    "dot.edge('q6','q6')\n",
    "dot.edge('q6','q7')\n",
    "dot.edge('q7','q7')\n",
    "dot.edge('q7','q8')\n",
    "dot.edge('q8','q8')\n",
    "dot.edge('q8','q9')\n",
    "dot.edge('q9','q9')\n",
    "dot.edge('q9','q10')\n",
    "dot.edge('q10','q10')\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word like *ask* would then be represented like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = graphviz.Digraph(\n",
    "    graph_attr={'rankdir':'LR'}\n",
    ")\n",
    "dot.node('q0',label='#',shape='circle')\n",
    "dot.node('q1',label='æ',shape='circle')\n",
    "dot.node('q2',label='æ',shape='circle')\n",
    "dot.node('q3',label='æ',shape='circle')\n",
    "dot.node('q4',label='s',shape='circle')\n",
    "dot.node('q5',label='s',shape='circle')\n",
    "dot.node('q6',label='s',shape='circle')\n",
    "dot.node('q7',label='k',shape='circle')\n",
    "dot.node('q8',label='k',shape='circle')\n",
    "dot.node('q9',label='k',shape='circle')\n",
    "dot.node('q10',label='#',shape='doublecircle')\n",
    "dot.node('s',style='invisible')\n",
    "dot.edge('s','q0')\n",
    "dot.edge('q0','q0')\n",
    "dot.edge('q0','q1')\n",
    "dot.edge('q1','q1')\n",
    "dot.edge('q1','q2')\n",
    "dot.edge('q2','q2')\n",
    "dot.edge('q2','q3')\n",
    "dot.edge('q3','q3')\n",
    "dot.edge('q3','q4')\n",
    "dot.edge('q4','q4')\n",
    "dot.edge('q4','q5')\n",
    "dot.edge('q5','q5')\n",
    "dot.edge('q5','q6')\n",
    "dot.edge('q6','q6')\n",
    "dot.edge('q6','q7')\n",
    "dot.edge('q7','q7')\n",
    "dot.edge('q7','q8')\n",
    "dot.edge('q8','q8')\n",
    "dot.edge('q8','q9')\n",
    "dot.edge('q9','q9')\n",
    "dot.edge('q9','q10')\n",
    "dot.edge('q10','q10')\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic step is to tie the distributions of states labeled the same together. In other words, when we train the HMM for *hat* that will update transitions and emissions. The emissions of the [æ] in *ask* will also get updated. When we train *ask*, the [æ] of *hat* will also get updated.\n",
    "\n",
    "What this means is that we need an iterated training procedure. Train all our HMMs in multiple passes so that the state tying effect can settle.\n",
    "\n",
    "What we need then is a library of distribution triples that we can drop into different HMMs.\n",
    "\n",
    "This is actually something we can do with *pomegranate*, but let's instead switch to a full HMM-based system to try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and running *Kaldi*\n",
    "\n",
    "You can install *Kaldi* on your own system, but it's *huge* with lots of moving parts and the easiest way to use it is via *docker*.\n",
    "\n",
    "On my own linux machine, I build the container like this:\n",
    "\n",
    "```bash\n",
    "docker run -it --gpus all --name kaldi \\\n",
    "  -v /data/:/mhdata/ \\\n",
    "  -v /home/hammond/speechtechbook/:/mhbook/ \\\n",
    "  -v /home/hammond/classesF23/:/mhclasses/ \\\n",
    "  kaldiasr/kaldi:gpu-latest bash\n",
    "```\n",
    "\n",
    "As usual, you need to change that for your own system:\n",
    "\n",
    "1. If you don't have a usable GPU, remove `--gpus all` and replace `gpu-latest` with `latest`.\n",
    "1. Change the `-v` lines as appropriate for your own system. The stuff on the left side of each colon is the location on your host system. The stuff on the right side is where you'll find that *within* the container.\n",
    "1. Make sure you include the location where you have the speech commands data.\n",
    "1. Make sure you include the location where you have the program files for the course text.\n",
    "1. Once the container is running for the first time, chack that your mounted directories are actually there. In other words, if you mounted a host directory at `/xyzdata`, make sure to type `ls /xyzdata` and confirm that you see your files.\n",
    "\n",
    "The official *Kaldi* image is missing some stuff, so you should execute the following commands within the container.\n",
    "\n",
    "```bash\n",
    "apt update\n",
    "apt install flac\n",
    "apt install gawk\n",
    "```\n",
    "\n",
    "*Kaldi* needs the *SRILM* toolkit and there is an install script in the `/opt/kaldi/tools` directory. Unfortunately, it doesn't work right, so here are the steps to make it work.\n",
    "\n",
    "1. `cd /opt/kaldi/tools`\n",
    "1. `./install_srilm.sh \"Mike Hammond\" \"University of Arizona\" \"hammond@u.arizona.edu\"`. Change the name and email as appropriate. *This will terminate with an error*.\n",
    "1. Remove the empty file `srilm.tar.gz`.\n",
    "1. Download SRILM from here: http://www.speech.sri.com/projects/srilm/download.html.\n",
    "1. Move that file into the `tools` directory and rename to `srilm.tar.gz`.\n",
    "1. Do the install script again: `./install_srilm.sh \"Mike Hammond\" \"University of Arizona\" \"hammond@u.arizona.edu\"` (again changing name and email as appropriate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *very* recipe\n",
    "\n",
    "The code files for the book include a directory called *very* with several obscure files in it. These files let you run the speech commands digits data with *Kaldi*.\n",
    "\n",
    "Copy that directory into `/opt/kaldi/egs/` and switch into it.\n",
    "\n",
    "There are a number of files there, but the important one is `r2.sh`. That file is in shell script and includes a definition near the top of the file for the variable `datadir`, which specifies where the speech commands data are.\n",
    "\n",
    "In my own case, that directory is called `commands` and it lives in `/data/`. When I built the container, I mounted `/data/` in the container as `/mhdata/`. *Therefore*, I define `datadir` in `r2.sh` as `/mhdata/commands/`. Edit the `r2.sh` file so it specifies the correct directory for your system.\n",
    "\n",
    "The other necessary change is the line five lines below the definition for `datadir` where `nj` is defined. This variable specifies the number of parallel jobs to run and you should set it according to the limits of your own system. In my own case, I have 8 dual-core processors, so I set it to 15, one less than the maximum. On my old mac, I have to set it to 3.\n",
    "\n",
    "Once you've made these changes, you can run the experiment by typing `./r2.sh`. On my linux machine, this takes 2-3 minutes, on my mac it's more like a half hour. With more limited resources, be prepared for a longer wait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the `r2.sh` script bit by bit.\n",
    "\n",
    "First, we define a bunch of variables:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "#kaldi run script for speech commands dataset\n",
    "#mike hammond, u. of arizona, 8/2021\n",
    "\n",
    "#define a bunch of variables\n",
    "datadir=/mhdata/commands/\n",
    "mfccdir=mfcc\n",
    "train_cmd=\"utils/run.pl\"\n",
    "decode_cmd=\"utils/run.pl\"\n",
    "nj=15\n",
    "lm_order=1\n",
    "trainnum=1000\n",
    "testnum=100\n",
    "\n",
    "#variable for digits to translate file names\n",
    "declare -a arr=(\"zero\" \"one\" \"two\" \"three\" \"four\"\n",
    "   \"five\" \"six\" \"seven\" \"eight\" \"nine\")\n",
    "```\n",
    "\n",
    "We call two other shell scripts that set paths and specify whether a GPU is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a bunch of files and directories where various files are placed as the body of the script runs.\n",
    "\n",
    "```bash\n",
    "#specify where programs are and how they interact\n",
    ". ./path.sh || exit 1\n",
    ". ./cmd.sh || exit 1\n",
    "\n",
    "#make data directory\n",
    "mkdir data\n",
    "#where to put the wave files\n",
    "mkdir data/wavefiles\n",
    "#make test directory\n",
    "mkdir data/test\n",
    "#make train directory\n",
    "mkdir data/train\n",
    "\n",
    "#create wav.scp (specify location of wav files)\n",
    "touch data/test/wav.scp\n",
    "touch data/train/wav.scp\n",
    "\n",
    "#rename and copy wave files\n",
    "echo copying wave files\n",
    "echo creating wav.scp, text, utt2spk files\n",
    "\n",
    "#make the text files (what's in each wav file)\n",
    "touch data/test/text\n",
    "touch data/train/text\n",
    "\n",
    "#make utt2spk files (specify speaker for each file)\n",
    "touch data/test/utt2spk\n",
    "touch data/train/utt2spk\n",
    "```\n",
    "\n",
    "Now we populate those directories. These files indicate where sound files are, who the speakers are, what files are for training and what for testing, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to create a *corpus* file which, in our case, is simply the list of digits.\n",
    "\n",
    "```bash\n",
    "#loop through the files doing all that\n",
    "for q in \"${arr[@]}\"\n",
    "do\n",
    "   filenames=`ls $datadir$q/*0.wav`\n",
    "   filenames=($filenames)\n",
    "   #training files\n",
    "   for filename in ${filenames[@]:0:${trainnum}}\n",
    "   do\n",
    "      speaker=`echo $filename | sed 's/.*\\///'`\n",
    "      speaker=`echo $speaker | sed 's/_.*//'`\n",
    "      pfx=\"${speaker}_${q}\"\n",
    "      cp $filename data/wavefiles/${pfx}.wav\n",
    "      echo ${pfx} data/wavefiles/${pfx}.wav >> data/train/wav.scp\n",
    "      echo ${pfx} ${q} >> data/train/text\n",
    "      echo -e \"${pfx} ${speaker}\" >> data/train/utt2spk\n",
    "   done\n",
    "   #testing files\n",
    "   for filename in ${filenames[@]:${trainnum}:${testnum}}\n",
    "   do\n",
    "      speaker=`echo $filename | sed 's/.*\\///'`\n",
    "      speaker=`echo $speaker | sed 's/_.*//'`\n",
    "      pfx=\"${speaker}_${q}\"\n",
    "      cp $filename data/wavefiles/${pfx}.wav\n",
    "      echo ${pfx} data/wavefiles/${pfx}.wav >> data/test/wav.scp\n",
    "      echo ${pfx} ${q} >> data/test/text\n",
    "      echo -e \"${pfx} ${speaker}\" >> data/test/utt2spk\n",
    "   done\n",
    "done\n",
    "\n",
    "#create corpus file (list all word tokens in wav files)\n",
    "echo Creating corpus file\n",
    "mkdir data/local\n",
    "touch data/local/corpus.txt\n",
    "\n",
    "for i in \"${arr[@]}\"\n",
    "do\n",
    "   echo $i >> data/local/corpus.txt\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run various scripts that check and fix the file structure if there are problems.\n",
    "\n",
    "We also move our own files from the `varylang` directory into the new directories.\n",
    "\n",
    "```bash\n",
    "#check/fix data directories\n",
    "echo Fixing, validating, sorting\n",
    "cp -r ../wsj/s5/utils .\n",
    "./utils/validate_data_dir.sh data/test\n",
    "./utils/fix_data_dir.sh data/test\n",
    "./utils/validate_data_dir.sh data/train\n",
    "./utils/fix_data_dir.sh data/train\n",
    "\n",
    "#copy trivial language model files\n",
    "echo Moving language model files\n",
    "mkdir data/local/dict\n",
    "cp verylang/*.txt data/local/dict\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next create the MFCC representations and do the cepstral mean and variance scaling. These require scripts from other experiments in the `egs` directory, so we just copy these over.\n",
    "\n",
    "```bash\n",
    "#making MFCCs\n",
    "echo Creating mfccs\n",
    "cp -r ../wsj/s5/steps .\n",
    "cp -r ../an4/s5/conf .\n",
    "\n",
    "#make MFCC log directories\n",
    "mkdir exp\n",
    "mkdir exp/make_mfcc\n",
    "mkdir exp/make_mfcc/train\n",
    "mkdir exp/make_mfcc/test\n",
    "\n",
    "#make the mfccs themselves\n",
    "steps/make_mfcc.sh --nj $nj --cmd \"$train_cmd\" \\\n",
    "   data/train exp/make_mfcc/train $mfccdir\n",
    "steps/make_mfcc.sh --nj $nj --cmd \"$train_cmd\" \\\n",
    "   data/test exp/make_mfcc/test $mfccdir\n",
    "\n",
    "#cepstral mean/variance statistics per speaker (cmvn)\n",
    "steps/compute_cmvn_stats.sh data/train \\\n",
    "   exp/make_mfcc/train $mfccdir\n",
    "steps/compute_cmvn_stats.sh data/test \\\n",
    "   exp/make_mfcc/test $mfccdir\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have a bunch of scripts that create the language model and massage it into the correct form. In our case, we have only words in isolation, so this is fairly trivial.\n",
    "\n",
    "```bash\n",
    "#prepare language data (trivial here)\n",
    "echo Preparing language data\n",
    "utils/prepare_lang.sh data/local/dict \"<UNK>\" \\\n",
    "   data/local/lang data/lang\n",
    "\n",
    "#build language model\n",
    "echo Building language model\n",
    "\n",
    "local=data/local\n",
    "\n",
    "#make arpa/binary version of LM\n",
    "mkdir $local/tmp\n",
    "../../tools/srilm/bin/i686-m64/ngram-count -order $lm_order \\\n",
    "   -write-vocab $local/tmp/vocab-full.txt -wbdiscount -text \\\n",
    "   $local/corpus.txt -lm $local/tmp/lm.arpa\n",
    "\n",
    "#make G.fst file (binary version for language model)\n",
    "lang=data/lang\n",
    "../../src/lmbin/arpa2fst --disambig-symbol=#0 \\\n",
    "   --read-symbol-table=$lang/words.txt $local/tmp/lm.arpa \\\n",
    "   $lang/G.fst\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next are the guts of the script. We do training based on concatenated segments. We then align and use these alignments to train again with context-dependent segments. Basically, we construct a version of each segment for every context it occurs in. There can be thousands of these, so the \"decision tree\" step winnows these down to just what we need.\n",
    "\n",
    "```bash\n",
    "#monophone/letter unigram training (HMM-GMMs)\n",
    "steps/train_mono.sh --nj $nj --cmd \"$train_cmd\" data/train \\\n",
    "   data/lang exp/mono || exit 1\n",
    "\n",
    "#mono decoding/testing\n",
    "\n",
    "#copy scripts\n",
    "cp -r ../an4/s5/local .\n",
    "\n",
    "#do decision trees\n",
    "utils/mkgraph.sh --mono data/lang exp/mono \\\n",
    "   exp/mono/graph || exit 1\n",
    "\n",
    "#score\n",
    "steps/decode.sh --config conf/decode.config --nj $nj --cmd \\\n",
    "   \"$decode_cmd\" exp/mono/graph data/test exp/mono/decode\n",
    "\n",
    "#mono alignment (for building triphones)\n",
    "steps/align_si.sh --nj $nj --cmd \"$train_cmd\" data/train \\\n",
    "   data/lang exp/mono exp/mono_ali || exit 1\n",
    "\n",
    "#triphone training (collect segment HMMs into triphones)\n",
    "steps/train_deltas.sh --cmd \"$train_cmd\" 2000 11000 data/train \\\n",
    "   data/lang exp/mono_ali exp/tri1 || exit 1\n",
    "\n",
    "#triphone decoding/testing\n",
    "\n",
    "#make decision trees\n",
    "utils/mkgraph.sh data/lang exp/tri1 exp/tri1/graph || exit 1\n",
    "\n",
    "#score\n",
    "steps/decode.sh --config conf/decode.config --nj $nj --cmd \\\n",
    "   \"$decode_cmd\" exp/tri1/graph data/test exp/tri1/decode\n",
    "\n",
    "echo '\n",
    "All done!'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the results with the following commands. The first one shows the word error rate (WER) for the initial single-segment model at each iteration of training. The second shows how this (generally) improves with context-dependent segments.\n",
    "\n",
    "```bash\n",
    "grep WER exp/mono/decode/wer*\n",
    "\n",
    "grep WER exp/tri1/decode/wer*\n",
    "```\n",
    "\n",
    "The system performs better than 90% depending on how much data you use for training, so this is a substantial improvement over the systems we've looked at so far"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5baff0486ed952ff4adb330b9f12e41af74b5b07ccf87626cb11b59d4d14f3d0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
